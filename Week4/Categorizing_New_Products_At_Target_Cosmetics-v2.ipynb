{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lnMVXWu2yoG"
   },
   "source": [
    "# Today you are a Machine Learning Engineer at the Department of New Products at Target Cosmetics!\n",
    "This work relies on processed data from Kaggle https://www.kaggle.com/mkechinov/ecommerce-events-history-in-cosmetics-shop\n",
    "\n",
    "This work is motivated by the publication https://arxiv.org/pdf/2010.02503.pdf\n",
    "\n",
    "### So far you have seen user-product interaction data that can lead to classification of a user-product relationship as ending in purchase or no-purchase, and for clustering (categorizing) user behaviors.\n",
    "### In this assignment, you have access to user-product level interactions without any insights into the user behaviors. Your goal is to classify if the \"Products\" will sell at least 5 pieces in a month (denoted by `Purchased?` =`1) or not. The intention is to utilize as minimum product level as possible (price and product category only) at first and then designing a more complex system that ingests more product level information.\n",
    "### Labeled data is sparse, and the intention is to maximize Recall (so that no popular cosmetic is understocked). Digital overstocking is allowed since it will not cause disengagement in customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wr4gFV2i2yoJ"
   },
   "outputs": [],
   "source": [
    "# The session-level data that is mined for this work is as follows:\n",
    "from IPython.display import Image\n",
    "Image(filename='image10.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRrXSVcM2yoO"
   },
   "source": [
    "## This week you are helping plan the launch of new products! You start with minimal product information and then identify what other information is helpful for the task!\n",
    "\n",
    "## The minimal product level information available to you about the new products is their cost range and product category (cream, foundation, lipcolor, etc..).\n",
    "\n",
    "## You have to figure out how to mine the past cosmetic sales data from last month, utilize relevant features and to make estimations as to which products will sell more (`Purchased?` = 1)\n",
    "\n",
    "## Task 0: Getting to know the Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJk12SE52yoP"
   },
   "outputs": [],
   "source": [
    "## Importing required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfwUwPtq2yoR"
   },
   "outputs": [],
   "source": [
    "# Load the data from previous months (past)\n",
    "Past = pd.read_csv(\"Past_month_products.csv\")\n",
    "print(Past.shape)\n",
    "Past.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vW5WsvMM2yoU"
   },
   "outputs": [],
   "source": [
    "# Next, load the data regarding products to be launched next month\n",
    "Next = pd.read_csv(\"Next_month_products.csv\")\n",
    "print(Next.shape)\n",
    "Next.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykrvRDWd2yoX"
   },
   "source": [
    "### Only the `product_id`, `maxPrice`, `minPrice`, and `Category` columns are common to both the training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_BUJ-Ex2yoX"
   },
   "source": [
    "# Task 1: Exploratory Data Analysis (EDA) and Data Preparation\n",
    "## EDA: Doing our your due diligence. Find the following:\n",
    "1. Percentage of Purchased events in train data: \n",
    "2. Percentage of Purchased events in test data:\n",
    "3. Are there any overlaps in product ID between train and test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-Vl3Tka2yoX"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "y_train = Past['Purchased?'].values\n",
    "print(f\"Percentage of Purchased in Training data = {(np.sum(y_train)/len(y_train))*100}\")\n",
    "y_test = Next['Purchased?'].values\n",
    "print(f\"Percentage of Purchased in Test data = {(np.sum(y_test)/len(y_test))*100}\")\n",
    "\n",
    "# Verify that every product ID in the training data appears only once\n",
    "print(f\"Every product ID in the training data appears only once: {len(np.unique(Past['product_id'])) == Past.shape[0]}\")\n",
    "# Verify that every product ID in the test data appears only once\n",
    "print(f\"Every product ID in the test data appears only once: {len(np.unique(Next['product_id'])) == Next.shape[0]}\")\n",
    "# Concatenate the product_id columns of the training and test DataFrames\n",
    "frames = [Past.iloc[:,0], Next.iloc[:,0]]\n",
    "result = np.array(pd.concat(frames))\n",
    "# Get all the unique product IDs and their counts\n",
    "prod, prod_counts = np.unique(result, return_counts=True)\n",
    "# Determine whether any product IDs appear in both the training and test data\n",
    "num = (prod_counts > 1).astype(int)\n",
    "overlap = set(Past['product_id']).intersection(set(Next['product_id']))\n",
    "print(f\"Number of product ids with count > 0 for training and test data combined = {sum(num)}\")\n",
    "print(f\"These product IDs are present in both the training and test data: {overlap}\")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAT34x3Otid3"
   },
   "source": [
    "## Next, create `X_train`, `y_train`, `X_test`, and `y_test`. Remember the following: \n",
    "1. The `Purchased?` column is the target\n",
    "2. `X_train` and `X_test` should contain the same features\n",
    "3. `product_id` should NOT be one of those features. Can you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4ltwfa62yoa"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "def return_train_test_data(df_old, df_new):\n",
    "    X_train = df_old[['maxPrice', 'minPrice', 'Category']].values\n",
    "    y_train = df_old[['Purchased?']].values\n",
    "    X_test  = df_new[['maxPrice', 'minPrice', 'Category']].values\n",
    "    y_test  = df_new[['Purchased?']].values\n",
    "    return X_train, y_train, X_test, y_test\n",
    "### END CODE HERE ###\n",
    "    \n",
    "X_train, y_train, X_test, y_test = return_train_test_data(Past, Next)    \n",
    "print(X_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOPpOWog2yoc"
   },
   "source": [
    "# Task 2, Baselining: Build the best classifier using the Past month's data that will predict if the Next month's products will be Purchased or not?\n",
    "## Consider using AutoML to estimate the best classifier. Which features would you use from the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xynRq9x4QlJL"
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line if using Colab\n",
    "# !pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Z0aR02P2yod"
   },
   "outputs": [],
   "source": [
    "# TPOT for classification\n",
    "from tpot import TPOTClassifier\n",
    "### START CODE HERE ###\n",
    "# Instantiate and train a TPOT auto-ML classifier\n",
    "# Set generations to 5, population_size to 40, and verbosity to 2 (so you can see each generation's performance)\n",
    "tpot = TPOTClassifier(generations=5, population_size=40, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "# Evaluate the classifier on the test data\n",
    "# By default, the scoring function is accuracy\n",
    "print(f\"{tpot.score(X_test, y_test)}\")\n",
    "### END CODE HERE ###\n",
    "tpot.export('tpot_products_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p6Ktoj02yof"
   },
   "source": [
    "<!-- ## Modify the file `tpot_products_pipeline.py` to return the prediction labels for `X_test` and paste the function here or reload kernel to reload updated file -->\n",
    "\n",
    "## Use the appropriate lines of `tpot_products_pipeline.py` (and modify the relevant names) to write a function which returns the predicted labels generated by the best classifier which TPOT found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wm5Ac97r2yog"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.svm import LinearSVC\n",
    "from tpot.builtins import StackingEstimator\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def return_tpot_results(X_train, y_train, X_test):\n",
    "    ### START CODE HERE ###\n",
    "    exported_pipeline = DecisionTreeClassifier(criterion=\"entropy\", max_depth=6, min_samples_leaf=19, min_samples_split=17)\n",
    "    \n",
    "    exported_pipeline.fit(X_train, y_train)\n",
    "    prediction = exported_pipeline.predict(X_test)\n",
    "    ### END CODE HERE ### \n",
    "    return prediction\n",
    "\n",
    "pred = return_tpot_results(X_train, y_train, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diQyYboWIb0u"
   },
   "source": [
    "## Evaluate the results of the best classifier which TPOT found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbGFSnzi2yoi"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "### START CODE HERE ###\n",
    "# TPOT confusion matrix\n",
    "cmtp = confusion_matrix(y_test, pred) \n",
    "acc  = accuracy(y_test, pred)\n",
    "rec  = recall(y_test, pred)\n",
    "prec = precision(y_test, pred)\n",
    "f1   = f1_score(y_test, pred)\n",
    "### END CODE HERE ###\n",
    "print(f'Accuracy = {acc}, Precision = {prec}, Recall = {rec}, F1-score = {f1}')\n",
    "print('Confusion Matrix is:')\n",
    "print(cmtp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-px8SPVv2yol"
   },
   "source": [
    "# Task 3, Semi-supervised learning: Apply label spreading on the data and run performance analysis by cross validation.\n",
    "\n",
    "Step 1: Combine `X_train` and `X_test`\n",
    "\n",
    "Step 2: Combine `y_train` and pad `y_test` with -1 labels\n",
    "\n",
    "Step 3: Run label spreading on complete data. Use knn spreading with `n_neighbors` varying as 1,3,5,7,9,11. What's the best neighborhood?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31i6GDQCJsdd"
   },
   "source": [
    "### Concatenate `X_train` and `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyxJD6So2yol"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "X = None\n",
    "### END CODE HERE ### \n",
    "print(X.shape[0])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtpeNzjQJvOy"
   },
   "source": [
    "### Create an array shaped like a column of `X_test`, with each value equal to -1\n",
    "### Make sure the array is a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBp058wd2yon"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "y_hat = None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta4rlU0qJzBo"
   },
   "source": [
    "### Concatenate `y_train` and `y_hat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK1QxliM2yop"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "y = None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ9YdmHQKFBB"
   },
   "source": [
    "### Instantiate and train the label-spreading model. Use a KNN kernel and set `alpha` to 0.01. Try the `n_neighbors` values mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ae91aPh82yor"
   },
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelSpreading\n",
    "### START CODE HERE ###\n",
    "lp_model = None\n",
    "None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUw9UyOxKRkS"
   },
   "source": [
    "### Extract the label predictions (transductions) for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOE-d_R02you"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "semi_sup_preds = None\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPyuxT3KKjDf"
   },
   "source": [
    "### Evaluate the test predictions against the true test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4FvLbbg2yow"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "cm   = None\n",
    "acc  = None\n",
    "rec  = None\n",
    "prec = None\n",
    "f1   = None\n",
    "### END CODE HERE ###\n",
    "print(f'Accuracy = {acc}, Precision = {prec}, Recall = {rec}, F1-score = {f1}')\n",
    "print('Confusion Matrix is:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3HziHwE2yoz"
   },
   "source": [
    "## Observe increase in recall by running label spreading. Tabulate your results\n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "Method    |   Recall      |F1-score    | Accuracy    |\n",
    "------------------------------------------------------------------------------\n",
    "AutoML    |                   |                    |                    |\n",
    "-------------------------------------------------------------------------\n",
    "### Label Spread |               |                        |                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Unmp-DRM2yoz"
   },
   "source": [
    "# Task 4, System Design for Zero Shot Learning:\n",
    "So far we have been looking at 3 product level features (min price, max price, Product Category) to classify if a particular product will get get purchased or not.\n",
    "Now, let's say you have access to some more information regarding each Past sold cosmetic item and the Next cosmetic item. Design a System to enable accurate identification of an item that is more likely to be purchased.\n",
    "Think through the following:\n",
    "1. What additional data fields do you need per cosmetic in past and Next catalogue? How would you process these data fields?\n",
    "2. You have access to picture images of each cosmetic. How will you use these images to extract relevant features for gauging interest in the new coemetics?\n",
    "3. Design an end-to-end system workflow using the additional cosmetic data and cosmetic images to predict its purchasing polularity. \n",
    "## Put the picture corresponding to the final System Diagram in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBreL_S_2yoz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Categorizing_New_Products_At_Target_Cosmetics-SVEN-v2.ipynb",
   "provenance": [
    {
     "file_id": "1b4Jdh2DtQhSL4Thk4pMKF2sBnKuNbqOr",
     "timestamp": 1605231801624
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
