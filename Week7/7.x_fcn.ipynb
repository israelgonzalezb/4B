{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsLFyDThqRL5"
   },
   "source": [
    "# Adapted from [Dive into Deep Learning](https://d2l.ai/index.html), Chapter 13, Section 11. The authors are prominent Amazon data scientists, so they work primarily with the [MXNet](https://mxnet.apache.org/versions/1.7.0/) framework rather than [TensorFlow](https://www.tensorflow.org/) or [PyTorch](https://pytorch.org/). As such, this assignment will rely on MXNet as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOq9YsRrxe3_"
   },
   "source": [
    "## If running this notebook in Google Colab, mount your Google Drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFS8ZUyJy_8H"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6A6y56BkUkb"
   },
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP17KMRFq-tY"
   },
   "source": [
    "## If running this notebook in Colab, make sure to navigate to Runtime > Change runtime type, select GPU in the Hardware accelerator drop-down menu, and click Save *before* running the following code cell! Otherwise, `mxnet-cu101` won't install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YqYDjFYkUkb"
   },
   "outputs": [],
   "source": [
    "#!pip install d2l==0.15.1\n",
    "#!pip install -U mxnet-cu101==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF9znR37kUkb",
    "origin_pos": 0
   },
   "source": [
    "# Fully Convolutional Networks (FCN)\n",
    ":label:`sec_fcn`\n",
    "\n",
    "We previously discussed semantic segmentation using each pixel in an image for\n",
    "category prediction. A fully convolutional network (FCN)\n",
    ":cite:`Long.Shelhamer.Darrell.2015` uses a convolutional neural network to\n",
    "transform image pixels to pixel categories. Unlike the convolutional neural\n",
    "networks previously introduced, an FCN transforms the height and width of the\n",
    "intermediate layer feature map back to the size of input image through the\n",
    "transposed convolution layer, so that the predictions have a one-to-one\n",
    "correspondence with input image in spatial dimension (height and width). Given a\n",
    "position on the spatial dimension, the output of the channel dimension will be a\n",
    "category prediction of the pixel corresponding to the location.\n",
    "\n",
    "We will first import the package or module needed for the experiment and then\n",
    "explain the transposed convolution layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "id": "eBgEOtxYkUkc",
    "origin_pos": 1,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, image, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWkwJ23kkUkc",
    "origin_pos": 2
   },
   "source": [
    "## Constructing a Model\n",
    "\n",
    "Here, we demonstrate the most basic design of a fully convolutional network model. As shown in :numref:`fig_fcn`, the fully convolutional network first uses the convolutional neural network to extract image features, then transforms the number of channels into the number of categories through the $1\\times 1$ convolution layer, and finally transforms the height and width of the feature map to the size of the input image by using the transposed convolution layer :numref:`sec_transposed_conv`. The model output has the same height and width as the input image and has a one-to-one correspondence in spatial positions. The final output channel contains the category prediction of the pixel of the corresponding spatial position.\n",
    "\n",
    "![Fully convolutional network. ](http://d2l.ai/_images/fcn.svg)\n",
    ":label:`fig_fcn`\n",
    "\n",
    "Below, we use a ResNet-18 model pre-trained on the ImageNet dataset to extract image features and record the network instance as `pretrained_net`. As you can see, the last two layers of the model member variable `features` are the global average pooling layer `GlobalAvgPool2D` and example flattening layer `Flatten`. The `output` module contains the fully connected layer used for output. These layers are not required for a fully convolutional network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-emWrouZIWL"
   },
   "source": [
    "### Exercise: Load a pretrained ResNet-18, v2 model from [`gluon.model_zoo.vision`](https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/gluon/model_zoo/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "id": "K6OXREilkUkc",
    "origin_pos": 3,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Load the pretrained model\n",
    "pretrained_net = None\n",
    "### END CODE HERE ###\n",
    "# Inspect the last 4 layers\n",
    "pretrained_net.features[-4:], pretrained_net.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4cHIPJCkUkd",
    "origin_pos": 4
   },
   "source": [
    "### Exercise: Instantiate a fully convolutional network and add all but the last two layers of `pretrained_net` to it\n",
    "\n",
    "### Next, we create the fully convolutional network instance `net`. It duplicates all the neural layers except the last two layers of the instance member variable `features` of `pretrained_net` and the model parameters obtained after pre-training.\n",
    "\n",
    "### Make sure to use [`HybridSequential()`](https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.HybridSequential) rather than [`Sequential()`](https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.Sequential). For a discussion of why, and the differences between the two approaches, go [here](https://gluon.mxnet.io/chapter07_distributed-learning/hybridize.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "id": "FSkZeEwvkUke",
    "origin_pos": 5,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Instantiate the FCN model\n",
    "net = None\n",
    "# Iterate through all but the last two layers\n",
    "for layer in None:\n",
    "    # Add the current layer to the FCN model\n",
    "    None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBzysE5tkUke",
    "origin_pos": 6
   },
   "source": [
    "Given an input of a height and width of 320 and 480 respectively, the forward computation of `net` will reduce the height and width of the input to $1/32$ of the original, i.e., 10 and 15.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "id": "mK31WGiBkUke",
    "origin_pos": 7,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "X = np.random.uniform(size=(1, 3, 320, 480))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSFBdxxrkUke",
    "origin_pos": 8
   },
   "source": [
    "Next, we transform the number of output channels to the number of categories of\n",
    "Pascal VOC2012 (21) through the $1\\times 1$ convolution layer. Finally, we need\n",
    "to magnify the height and width of the feature map by a factor of 32 to change\n",
    "them back to the height and width of the input image. Recall the calculation\n",
    "method for the convolution layer output shape described in\n",
    ":numref:`sec_padding`. Because\n",
    "$(320-64+16\\times2+32)/32=10$ and $(480-64+16\\times2+32)/32=15$, we construct a\n",
    "transposed convolution layer with a stride of 32 and set the height and width of\n",
    "the convolution kernel to 64 and the padding to 16. It is not difficult to see\n",
    "that, if the stride is $s$, the padding is $s/2$ (assuming $s/2$ is an integer),\n",
    "and the height and width of the convolution kernel are $2s$, the transposed\n",
    "convolution kernel will magnify both the height and width of the input by a\n",
    "factor of $s$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kf2cf6HcFMB"
   },
   "source": [
    "### Exercise: Add the last two layers to the FCN model. For the first of these, use a 1 x 1 (2D) convolution with 21 output channels, since the Pascal VOC2012 (21) segmentation dataset has 21 classes. For the second, use a transposed 2D convolutional layer with a stride of 32, kernel size of 64, and padding of 16 pixels so that we reverse the size reduction which takes place when an image is passed through the previous model layers.\n",
    "\n",
    "### You might find [`nn.Conv2D()`](https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.Conv2D) and [`nn.Conv2DTranspose()`](https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.Conv2DTranspose) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "id": "E-wSfBYPkUke",
    "origin_pos": 9,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Set the number of classes\n",
    "num_classes = None\n",
    "# Add the 2D convolutional layer and the transposed 2D convolutional layer\n",
    "# with the parameters given above\n",
    "net.add(None,\n",
    "        None)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2Cp787LkUke",
    "origin_pos": 10
   },
   "source": [
    "## Initializing the Transposed Convolution Layer\n",
    "\n",
    "We already know that the transposed convolution layer can magnify a feature map. In image processing, sometimes we need to magnify the image, i.e., upsampling. There are many methods for upsampling, and one common method is bilinear interpolation. Simply speaking, in order to get the pixel of the output image at the coordinates $(x, y)$, the coordinates are first mapped to the coordinates of the input image $(x', y')$. This can be done based on the ratio of the size of three input to the size of the output. The mapped values $x'$ and $y'$ are usually real numbers. Then, we find the four pixels closest to the coordinate $(x', y')$ on the input image. Finally, the pixels of the output image at coordinates $(x, y)$ are calculated based on these four pixels on the input image and their relative distances to $(x', y')$. Upsampling by bilinear interpolation can be implemented by transposed convolution layer of the convolution kernel constructed using the following `bilinear_kernel` function. Due to space limitations, we only give the implementation of the `bilinear_kernel` function and will not discuss the principles of the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "id": "ZaKrTRoYkUke",
    "origin_pos": 11,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (np.arange(kernel_size).reshape(-1, 1),\n",
    "          np.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - np.abs(og[0] - center) / factor) * \\\n",
    "           (1 - np.abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return np.array(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3_t_vXakUke",
    "origin_pos": 12
   },
   "source": [
    "### Exercise: For illustrative purposes, we will experiment with bilinear interpolation upsampling implemented by transposed convolution layers. Construct a transposed convolution layer that magnifies height and width of input by a factor of 2 and initialize its convolution kernel with the `bilinear_kernel` function.\n",
    "\n",
    "### You may find the [`initialize()`](https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.Block.initialize) method for [`nn.Block()`](https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.Block) (i.e. the base class of all `nn` layers) and `init.Constant()` (which seems to be an alias for [`mxnet.initializer.Constant()`](https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/initializer/index.html#mxnet.initializer.Constant)) useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "id": "xIzUTW_1kUke",
    "origin_pos": 13,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Instantiate the illustrative transposed convolutional layer\n",
    "# Make sure this layer has 3 output channels and the necessary parameters for an image enlargmenet factor of 2\n",
    "conv_trans = None\n",
    "# Use the output of a bilinear kernel with 3 input and output channels and a kernel size of 4\n",
    "# to initialize the layer\n",
    "None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_0_A5yrkUke",
    "origin_pos": 14
   },
   "source": [
    "### Exercise: Apply the illustrative transposed convolutional layer to the picture of the cat and dog\n",
    "Read the image `X` and record the result of upsampling as `Y`. In order to print the image, we need to adjust the position of the channel dimension. After reading in the image, preprocess it as follows: \n",
    "1. Ensure that the image's pixel values are represented as 32-bit floating point numbers\n",
    "2. Transpose its dimensions so that the channel dimension comes first\n",
    "3. Add an initial dimension corresponding to batch size (1 in this case)\n",
    "4. Finally, normalize the image so its pixel values fall in the range [0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iECSJ6v6kUke",
    "origin_pos": 15,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "# Change the path to the catdog.jpg picture if necessary\n",
    "img = image.imread('/content/drive/MyDrive/Colab Notebooks/Week 7 Offline Assignment - FCN/catdog.jpg')\n",
    "### START CODE HERE ###\n",
    "# Preprocess the image according to the instructions above\n",
    "X = None\n",
    "# Apply the transposed convolutional layer to the preprocessed image\n",
    "Y = None\n",
    "# Transpose the dimensions of the 0th (and only) image in the batch so channels come last\n",
    "out_img = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idFlOSFjkUke",
    "origin_pos": 16
   },
   "source": [
    "As you can see, the transposed convolution layer magnifies both the height and width of the image by a factor of 2. It is worth mentioning that, besides to the difference in coordinate scale, the image magnified by bilinear interpolation and original image printed in :numref:`sec_bbox` look the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qg62eSFkUke",
    "origin_pos": 17,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "print('input image shape:', img.shape)\n",
    "d2l.plt.imshow(img.asnumpy());\n",
    "print('output image shape:', out_img.shape)\n",
    "d2l.plt.imshow(out_img.asnumpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJNFlgrmkUkh",
    "origin_pos": 18
   },
   "source": [
    "### Exercise: Initialize the final 2 layers of the FCN model\n",
    "In a fully convolutional network, we initialize the transposed convolution layer for upsampled bilinear interpolation. For a $1\\times 1$ convolution layer, we use Xavier for random initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "id": "mFHC11m6kUkh",
    "origin_pos": 19,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Make a bilinear kernel with which to initialize the final transposed convolutional layer\n",
    "# Remember, the number of input and output channels is the number of classes \n",
    "# and we want to enlarge the final images by a factor of 32\n",
    "W = None\n",
    "# Initialize the final layer with W\n",
    "None\n",
    "# Initialize the penultimate layer with the Xavier initialization\n",
    "None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6gHDQnZkUkh",
    "origin_pos": 20
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "We read the dataset using the method described in the previous section. Here, we specify shape of the randomly cropped output image as $320\\times 480$, so both the height and width are divisible by 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "id": "GAB4DL7EkUkh",
    "origin_pos": 21,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size, crop_size = 32, (320, 480)\n",
    "train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxX_rQJtkUkh",
    "origin_pos": 22
   },
   "source": [
    "## Training\n",
    "\n",
    "Now we can start training the model. The loss function and accuracy calculation here are not substantially different from those used in image classification. Because we use the channel of the transposed convolution layer to predict pixel categories, the `axis=1` (channel dimension) option is specified in `SoftmaxCrossEntropyLoss`. In addition, the model calculates the accuracy based on whether the prediction category of each pixel is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "id": "A9LlrHcOkUkh",
    "origin_pos": 23,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs, lr, wd, devices = 5, 0.1, 1e-3, d2l.try_all_gpus()\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis=1)\n",
    "net.collect_params().reset_ctx(devices)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                        {'learning_rate': lr, 'wd': wd})\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmO-fBWYkUkh",
    "origin_pos": 24
   },
   "source": [
    "## Prediction\n",
    "\n",
    "During predicting, we need to standardize the input image in each channel and transform them into the four-dimensional input format required by the convolutional neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "id": "mlhxB950kUkh",
    "origin_pos": 25,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    X = test_iter._dataset.normalize_image(img)\n",
    "    X = np.expand_dims(X.transpose(2, 0, 1), axis=0)\n",
    "    pred = net(X.as_in_ctx(devices[0])).argmax(axis=1)\n",
    "    return pred.reshape(pred.shape[1], pred.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcMwAMM9kUkh",
    "origin_pos": 26
   },
   "source": [
    "To visualize the predicted categories for each pixel, we map the predicted categories back to their labeled colors in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "id": "VRcwCy3NkUkh",
    "origin_pos": 27,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "def label2image(pred):\n",
    "    colormap = np.array(d2l.VOC_COLORMAP, ctx=devices[0], dtype='uint8')\n",
    "    X = pred.astype('int32')\n",
    "    return colormap[X, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNaS3JiVkUkh",
    "origin_pos": 28
   },
   "source": [
    "The size and shape of the images in the test dataset vary. Because the model uses a transposed convolution layer with a stride of 32, when the height or width of the input image is not divisible by 32, the height or width of the transposed convolution layer output deviates from the size of the input image. In order to solve this problem, we can crop multiple rectangular areas in the image with heights and widths as integer multiples of 32, and then perform forward computation on the pixels in these areas. When combined, these areas must completely cover the input image. When a pixel is covered by multiple areas, the average of the transposed convolution layer output in the forward computation of the different areas can be used as an input for the softmax operation to predict the category.\n",
    "\n",
    "For the sake of simplicity, we only read a few large test images and crop an area with a shape of $320\\times480$ from the top-left corner of the image. Only this area is used for prediction. For the input image, we print the cropped area first, then print the predicted result, and finally print the labeled category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "id": "FVUwHeh9kUkh",
    "origin_pos": 29,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\n",
    "test_images, test_labels = d2l.read_voc_images(voc_dir, False)\n",
    "n, imgs = 4, []\n",
    "for i in range(n):\n",
    "    crop_rect = (0, 0, 480, 320)\n",
    "    X = image.fixed_crop(test_images[i], *crop_rect)\n",
    "    pred = label2image(predict(X))\n",
    "    imgs += [X, pred, image.fixed_crop(test_labels[i], *crop_rect)]\n",
    "d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV7o5NnikUkh",
    "origin_pos": 30
   },
   "source": [
    "## Summary\n",
    "\n",
    "* The fully convolutional network first uses the convolutional neural network to extract image features, then transforms the number of channels into the number of categories through the $1\\times 1$ convolution layer, and finally transforms the height and width of the feature map to the size of the input image by using the transposed convolution layer to output the category of each pixel.\n",
    "* In a fully convolutional network, we initialize the transposed convolution layer for upsampled bilinear interpolation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "7.x_fcn.ipynb",
   "provenance": [
    {
     "file_id": "16X__r3Gk1sre9I9ry1_50UjswwgnPpNo",
     "timestamp": 1606781767435
    },
    {
     "file_id": "https://github.com/d2l-ai/d2l-en-colab/blob/master/chapter_computer-vision/fcn.ipynb",
     "timestamp": 1606328700536
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
